
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1

cd scaromare/MontePi/singleNode/singleGpu/multiGpuTest/
git add Makefile
git commit -m "Add Rootbeer.jar to dependency"
git add .
git commit -m "add automatic kernel count chooser and quickfix for memory-out-of-bound error by rootbeer"
cd ../multiGpuTestSpark/
git add .
echo git commit -m "simple multi-GPU example which works in Spark"
git add ../../../../rootbeer1
git commit -m "simple multi-GPU example which works in Spark"
salloc -p gpu2-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1

git status
git diff
git diff TestMonteCarloPi.scala
git add TestMonteCarloPi.scala
git commit -m "avoid seed ranges of width 3 or so because of incorrect overflow"
git add -p MonteCarloPi.scala
git commit -m "measure exact memory needed on 2xK80 node"
git add Makefile
git commit -m "Add .jar's as dependencies"
git commit -a -m "Old Spark map working now, added more log output"
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 824300001 1 2>/dev/null
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=00:30:00 --mem-per-cpu=1200M
salloc -p gpu2-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=00:30:00 --mem-per-cpu=1200M
cd ~/scaromare/MontePi/singleNode/singleGpu/multiGpuTest
salloc -p gpu2-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
salloc -p gpu2-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
cd ../multiGpuTestSpark/
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
salloc -p gpu2-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
git commit -a -m "further things to try out
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
ca /home/h0/$USER/scaromare/MontePi/singleNode/singleGpu/multiGpuTestSpark/hs_err_pid65418.log
cat /home/h0/$USER/scaromare/MontePi/singleNode/singleGpu/multiGpuTestSpark/hs_err_pid65418.log
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
git commit -a -m "missing println"
cd scaromare/MontePi/singleNode/singleGpu/multiGpuTestSpark/
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=1:00:00 --mem-per-cpu=1200M

cd scaromare/rootbeer1/
ant jar && ./pack-rootbeer
git diff src/org/trifort/rootbeer/runtime/CUDAContext.java
git add -p src/org/trifort/rootbeer/runtime/CUDAContext.java
git commit -m "move signal back into each case, although I'm not sure it should make a difference. Only if the value is an unknown even command code"
git add -p src/org/trifort/rootbeer/runtime/CUDAContext.java
git commit -m "debug unknown GPU events"
git add -p src/org/trifort/rootbeer/runtime/CUDAContext.java
ant jar && ./pack-rootbeer
git diff src/org/trifort/rootbeer/runtime/CUDAContext.java  | grep -i -A 20 readcubin
ant jar && ./pack-rootbeer
git add src/org/trifort/rootbeer/runtime/*.java
git reset src/org/trifort/rootbeer/runtime/Serializer.java
git commit -m "style and add setAddress(0)" --edit
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
git add csrc/org/trifort/rootbeer/runtime/FixedMemory.*
git commit -m "Condense code for better readability and bug-safety"
git diff src/org/trifort/rootbeer/runtime/
git checkout HEAD csrc/org/trifort/rootbeer/runtime/FixedMemory.g
git checkout HEAD csrc/org/trifort/rootbeer/runtime/FixedMemory.h
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
git checkout HEAD~1 csrc/org/trifort/rootbeer/runtime/FixedMemory.h
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
git checkout HEAD~1 csrc/org/trifort/rootbeer/runtime/FixedMemory.c
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
git add csrc/org/trifort/rootbeer/runtime/FixedMemory.h
git checkout HEAD csrc/org/trifort/rootbeer/runtime/FixedMemory.c
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
git add csrc/org/trifort/rootbeer/runtime/FixedMemory.c
git commit --amend --no-edit
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
git add csrc/org/trifort/rootbeer/runtime/FixedMemory.c src/org/trifort/rootbeer/runtime/CUDAContext.java  src/org/trifort/rootbeer/runtime/CUDALoader.java   src/org/trifort/rootbeer/runtime/FixedMemory.java  src/org/trifort/rootbeer/runtime/Rootbeer.java
git commit -m "trying to understand serialization"
git add README.md

cd scaromare/rootbeer1/
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
cd scaromare/MontePi/singleNode/singleGpu/multiGpuTestSpark/
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=00:20:00 --mem-per-cpu=1200M
la
cd scaromare/MontePi/singleNode/singleGpu/
cd multiGpuTestSpark/
la
make -B SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
make -B SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
exit
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 1
srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 2
exit
make -B SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib

cd scaromare/rootbeer1/
git pull
( cd csrc && ./compile_linux_x64 ) && ant jar && ./pack-rootbeer
firefox http://172.24.36.106:8080
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib && srun /home/$USER/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --master local[1] --class TestMonteCarloPi MontePi.jar 26843 2

cd scaromare/MontePi/
rm -- '-singleNode-singleCore-java.log'
mv *.log *.dat raw-logs/
./benchmarkTaurusScaling.sh
la /home/$USER/scaromare/MontePi/singleNode/singleGpu/multiGpuTestSpark/MontePi.jar
./benchmarkTaurusScaling.sh
scancel -u $USER
cd singleNode/singleGpu/multiGpuTestSpark/
make SPARK_ROOT=~/spark-1.5.2-bin-hadoop2.6 SPARKCORE_JAR=~/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar SCALA_ROOT=$(dirname $(which scala))/../lib
salloc -p gpu1-interactive --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:2 --time=00:20:00 --mem-per-cpu=1200M
up 3
./benchmarkTaurusScaling.sh
scancel -u $USER
